---
title: "R Project: Denver Public School Rating"
author: "Ann Hu & Kexin Shang"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: columns
    #vertical_layout: fill
    vertical_layout: scroll
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(xlsx)
library(caret)
library(glmnet)
library(gbm)
library(GGally)
library(ranger)
library(rpart)
library(ggplot2)
library(nnet)
library(keras)
library(MASS)
theme_set(theme_bw()) #set ggplot as black and white
library(flexdashboard)
library(leaflet)
```


Map
=============================================================================================================================

### Our project analyzed ratings of 119 public schools in the Denver city.  

***

This map shows the Denver public schools contained in our data set. 

- Marker color indicate school's rating range:  
  Green - High  
  Orange - Medium  
  Red - Low  

- Hover over a marker to see the school name and rating. 


```{r}
library(leaflet)
library(htmltools)

# read in the geo info of schcools
dfMap <- read.xlsx("School.xlsx", sheetName = "Map")

# customize marker color based on school rating
getColor <- function(dfMap) {
  sapply(dfMap$Rating, function(Rating) {
  if(Rating <= 30) {
    "red"
  } else if(Rating <= 50) {
    "orange"
  } else {
    "green"
  } })
}

icons <- awesomeIcons(
  iconColor = "white",
  markerColor = getColor(dfMap)
)

# build map
leaflet(dfMap) %>%
  addTiles() %>%
  addAwesomeMarkers(~Long, ~Lat, icon = icons, label = ~htmlEscape(Name_Rating))
```

 

Model Details 
=========================================================================================================

### A regression analysis of multiple models was conducted to estimate the rating of the schools and identify the significant predictors impating the ratings.

```{r, include=FALSE}
df <- read.xlsx("School.xlsx", sheetName = "Summary-nonzero")
set.seed(2020)
inTraining <- caret::createDataPartition(df$Rating, 
                                         p = .8,
                                         list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]
```

### Multilinear Regression
```{r}
lm_reg <- lm(Rating ~ ., data = training)
test_preds <- predict(lm_reg, newdata = testing)
test_df <- testing %>%
  mutate(y_hat_lm = test_preds,
         sq_err_lm = (y_hat_lm - Rating)^2)
lm_MSE <- mean(test_df$sq_err_lm)
summary(lm_reg)
```

### Lasso Model
```{r}
x <- model.matrix(Rating ~ ., df)[, -1]
y <- df$Rating
x_train <- x[inTraining, ]
x_test  <- x[-inTraining, ]
y_train <- y[inTraining]
y_test <- y[-inTraining]

set.seed(2020)
lambdas <- 10^seq(-2, 5, len = 100)
lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = lambdas)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambdas)
test_preds <- predict(lasso_mod, s = lasso_cv$lambda.min, newx = x_test)
test_df <- test_df %>%
  mutate(y_hat_lasso = test_preds,
         sq_err_lasso = (test_preds - Rating)^2)

lasso_full <- glmnet(x,y,alpha = 1)
coef_lasso <- coef(lasso_full,s = lasso_cv$lambda.min)
coef_lasso
```

### Regression Tree
```{r}
set.seed(2020)
train_control <- trainControl(method = "repeatedcv",
                            number = 10, 
                            repeats = 5)
tune_grid <- data.frame(maxdepth = 1:10) #num of branches
regTree <- train(Rating ~ ., 
                        data = training,
                        method = "rpart2", 
                        trControl = train_control,
                        tuneGrid = tune_grid)
test_preds <- predict(regTree, newdata = testing)
test_df <- test_df %>%
  mutate(y_hat_regTree = test_preds,
         sq_err_regTree = (test_preds - Rating)^2)
regTree_MSE <- mean(test_df$sq_err_regTree)
plot(regTree)
```

### Random Forest
```{r}
set.seed(2020)
tune_grid <- expand.grid(mtry = 1:16,
                        splitrule = "variance",
                        min.node.size = 10)
train_control <- trainControl(method = "cv", number = 10)
rf <- train(Rating ~ .,
                    data = training,
                    method = "ranger",
                    num.trees = 500,
                    importance = "impurity",
                    trControl = train_control,
                    tuneGrid = tune_grid)
test_preds <- predict(rf, newdata = testing)
test_df <- test_df %>%
  mutate(y_hat_rf = test_preds,
         sq_err_rf = (test_preds - Rating)^2)
rf_MSE <- mean(test_df$sq_err_rf)
plot(rf)
```

### Gradient Boosting

```{r}
set.seed(2020)
grid <- expand.grid(interaction.depth = c(1:10), #how many branches in the tree
                    n.trees = seq(100, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)

trainControl <- trainControl(method = "cv", number = 5)

gbm <- train(Rating ~ ., 
                    data = training, 
                    distribution = "gaussian", 
                    method = "gbm",
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
test_preds <- predict(gbm, newdata = testing)
test_df <- test_df %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (test_preds - Rating)^2)
gbm_MSE <- mean(test_df$sq_err_gbm)
plot(gbm)
```


### Nerual Network

```{r}
set.seed(2020)
tune_grid <- expand.grid(size = 1:8,
                         decay = 10^seq(-4, -2, len=6))

fit_control <- trainControl(method = "cv",
                           number = 10)

nn_train<- train(Rating ~ .,
                         data = training,
                         method = "nnet",
                         tuneGrid = tune_grid,
                         trControl = fit_control,
                         maxit = 500,
                         linout = TRUE,
                         verbose = FALSE,
                         trace = FALSE)

nn <- nnet(Rating ~ .,
                     data = training,
                     size =7,
                     decay = 0.003981072,
                     linout = TRUE,
                     maxit = 500,
                     trace = FALSE)

test_preds <- predict(nn, newdata = testing)
test_df <- test_df %>%
  mutate(y_hat_nn = test_preds,
         sq_err_nn = (test_preds - Rating)^2)
nn_MSE <- mean(test_df$sq_err_nn)

print("The final values used for the model were size = 7 and decay = 0.003981072")
```

### Deep Learning

```{r}
x_train <- df[inTraining, ] %>%
  dplyr::select(-Rating)
x_test  <- df[-inTraining, ] %>%
  dplyr::select(-Rating)
y_train <- df[inTraining, ] %>%
  dplyr::select(Rating) %>%
  .[[1]]    
y_test <- df[-inTraining, ] %>%
  dplyr::select(Rating) %>%
  .[[1]]

build_network <- function(){
  network <- keras_model_sequential() %>%
    layer_dense(units = 64,
                activation = "relu",
                input_shape = dim(x_train)[[2]]) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  
  network %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("mae")
  )
}

set.seed(2020)
k <- 10
indices <- sample(1:y_train, replace = F)
folds <- cut(indices, breaks = k, labels = F)

num_epochs <- 100
accuracy_histories <- matrix(NA, nc = num_epochs, nr = k)

for(i in 1:k){
  # i <- 1
  # cat(sprintf('\nProcessing Fold: %s at %s\n', i, Sys.time()))
  val_index <- which(folds == i, arr.ind = TRUE)
  x_val <- data.matrix(x_train[val_index, ])
  y_val <- y_train[val_index]
  x_train_sub <- data.matrix(x_train[-val_index, ])
  y_train_sub <- y_train[-val_index]
  model <- build_network()
  history <- model %>% fit(x_train_sub, 
                           y_train_sub, 
                           epochs = num_epochs, 
                           batch_size = 20,
                           verbose = 0,
                           validation_data = list(x_val, y_val)) 
  # results <- history$metrics$val_loss
  accuracy_histories[i, ] <- history$metrics$val_loss
}
  avg_accuracy <- tibble(epoch = 1:num_epochs,
                       mse = apply(accuracy_histories, 2, mean))
p <- ggplot(data = avg_accuracy,
            aes(x = epoch, y = mse))
p + geom_point() +
  geom_line() + 
  ylim(0, 50) +
  geom_smooth() +
  theme_bw()

bestEpoch <- avg_accuracy[avg_accuracy$mse == min(avg_accuracy$mse),"epoch"]
print(paste("Best number of epoch:", bestEpoch))
```
```{r}
network <- build_network()

network %>% fit(data.matrix(x_train), 
                y_train, 
                epochs = 80, 
                batch_size = 20,
                verbose = 0,
                validation_data = list(data.matrix(x_test), y_test))
test_preds <- network %>%
  predict(data.matrix(x_test))
test_df <- test_df %>%
  mutate(y_hat_deep = test_preds,
         sq_err_deep = (test_preds - Rating)^2)
deep_MSE <- mean(test_df$sq_err_deep)
```

Model Comparison
============================================================================================================

### Predicted Rating of all models

```{r}
p_yhat <- dplyr::select(test_df, Rating, y_hat_lm, y_hat_lasso, y_hat_regTree, 
                   y_hat_rf, y_hat_gbm, y_hat_nn, y_hat_deep) %>%
  tidyr::gather(method, prediction) %>%
  ggplot(aes(prediction, fill = method))
p_yhat + geom_density(alpha = .2) +
  facet_wrap(~ method) +
  scale_fill_brewer(palette = "Dark2")
```

### Square error of all models

```{r}
p_sqerr <- dplyr::select(test_df, Rating, sq_err_lm, sq_err_lasso, sq_err_regTree, 
                   sq_err_rf, sq_err_gbm, sq_err_nn, sq_err_deep) %>%
  tidyr::gather(method, sq_err) %>%
  ggplot(aes(sq_err, fill = method))
p_sqerr + geom_density(alpha = .2) +
  scale_x_continuous("squared error", limits = c(0, 50)) +
  scale_fill_brewer(palette = "Dark2",
                    labels = c("Rating", "lm", "lasso" , "tree", "rf", "gbm", "nn", "deep")) 

```

### Test MSE of all models
```{r}
dfsqerr <- data.frame(p_sqerr$data)
TestMSE <- dfsqerr %>% 
  dplyr::group_by(method) %>%
  dplyr::summarize(err_mean = mean(sq_err),     
                   err_std = sd(sq_err))
TestMSE
```


Predictor Importance
=============================================================================================================================

### Predictor Importance by random forest
```{r}
imp <- varImp(rf)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn,
                      importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
coord_flip()
```

### Predictor Importance by Gradient Boosting
```{r}
imp <- varImp(gbm)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

Conclusion & Recommendation
==============================================================================================================================
